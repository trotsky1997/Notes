# Denoising Diffusion Probabilistic Models
> [!info]+ <center>Metadata</center>
> 
> |<div style="width: 5em">Key</div>|Value|
> |--:|:--|
> |æ–‡çŒ®ç±»å‹|preprint|
> |æ ‡é¢˜|Denoising Diffusion Probabilistic Models|
> |çŸ­æ ‡é¢˜||
> |ä½œè€…|[[Jonathan Ho]]ã€ [[Ajay Jain]]ã€ [[Pieter Abbeel]]|
> |æœŸåˆŠåç§°||
> |DOI||
> |å­˜æ¡£ä½ç½®||
> |é¦†è—ç›®å½•|arXiv.org|
> |ç´¢ä¹¦å·||
> |ç‰ˆæƒ||
> |åˆ†ç±»||
> |æ¡ç›®é“¾æ¥|[My Library](zotero://select/library/items/HIEWLPRH)|
> |PDF é™„ä»¶|<ul><li><a href="zotero://open-pdf/library/items/RZEREA2F">å…¨æ–‡</a></li><li><a href="zotero://open-pdf/library/items/S3SYFGSM">Full Text PDF</a></li></ul>|
> |å…³è”æ–‡çŒ®||
> ^Metadata

> [!example]- <center>æœ¬æ–‡æ ‡ç­¾</center>
> 
> `$=dv.current().file.tags`

> [!quote]- <center>Abstract</center>
> 
> We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion

> [!tldr]- <center>éšè—ä¿¡æ¯</center>
> 
> itemType:: preprint
> title:: Denoising Diffusion Probabilistic Models
> shortTitle:: 
> creators:: [[Jonathan Ho]]ã€ [[Ajay Jain]]ã€ [[Pieter Abbeel]]
> publicationTitle:: 
> journalAbbreviation:: 
> volume:: 
> issue:: 
> pages:: 
> language:: 
> DOI:: 
> ISSN:: 
> url:: [http://arxiv.org/abs/2006.11239](http://arxiv.org/abs/2006.11239)
> archive:: 2023-07-12
> archiveLocation:: 
> libraryCatalog:: arXiv.org
> callNumber:: 
> rights:: 
> extra:: ğŸ·ï¸ ğŸ“’ã€Computer Science - Machine Learningã€Statistics - Machine Learning
> collection:: 
> tags:: #Done #Computer_Science_-_Machine_Learning #Statistics_-_Machine_Learning
> related:: 
> itemLink:: [My Library](zotero://select/library/items/HIEWLPRH)
> pdfLink:: <ul><li><a href="zotero://open-pdf/library/items/RZEREA2F">å…¨æ–‡</a></li><li><a href="zotero://open-pdf/library/items/S3SYFGSM">Full Text PDF</a></li></ul>
> qnkey:: 2020_Ho_Denoising Diffusion _KEY-HIEWLPRH
> date:: 2020-12-16
> dateY:: 2020
> dateAdded:: 2023-07-09
> dateModified:: 2023-07-13
> 
> abstract:: We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at httpsï¼š//github.com/hojonathanho/diffusion


%--------------Ï‰--------------%

## âœï¸ ç¬”è®°åŒº

> [!WARNING]+ <center>ğŸ£ æ€»ç»“</center>  
>
>ğŸ¯ ç ”ç©¶é—®é¢˜::  
ğŸ” ç ”ç©¶èƒŒæ™¯::  
ğŸš€ ç ”ç©¶æ–¹æ³•::  
ğŸ” ç ”ç©¶æ€è·¯::  
ğŸ“º ä¸»è¦å†…å®¹::  
ğŸ‰ ç ”ç©¶ç»“è®º::  
ğŸ—ï¸ åˆ›æ–°ç‚¹::  
ğŸ’© ç ”ç©¶å±€é™::  
ğŸ¾ ç ”ç©¶å±•æœ›::  
âœï¸ å¤‡æ³¨::  

> [!inbox]- <center>ğŸ“« å¯¼å…¥æ—¶é—´</center>
>
> â° importDate:: 2023-07-10

%--------------Ï‰--------------%

## ğŸ“ æ³¨é‡Šç¬”è®° RZEREA2F

> <span style="font-size: 15px;color: gray">ğŸ“ 2020-Ho-Denoising Diffusion Probabilistic Models</span>

^KEYrefTitle

> <span class="highlight" style="background-color: #aaaaaa">This paper presents progress in diffusion probabilistic models [53]. A diffusion probabilistic model (which we will call a â€œdiffusion modelâ€ for brevity) is a parameterized Markov chain trained using variational inference to produce samples matching the data after finite time. Transitions of this chain are learned to reverse a diffusion process, which is a Markov chain that gradually adds noise to the data in the opposite direction of sampling until signal is destroyed. When the diffusion consists of small amounts of Gaussian noise, it is sufficient to set the sampling chain transitions to conditional Gaussians too, allowing for a particularly simple neural network parameterization.</span>  
> ğŸ”¤æœ¬æ–‡ä»‹ç»äº†æ‰©æ•£æ¦‚ç‡æ¨¡å‹çš„è¿›å±•[53]ã€‚æ‰©æ•£æ¦‚ç‡æ¨¡å‹ï¼ˆä¸ºç®€æ´èµ·è§ï¼Œæˆ‘ä»¬å°†å…¶ç§°ä¸ºâ€œæ‰©æ•£æ¨¡å‹â€ï¼‰æ˜¯ä½¿ç”¨å˜åˆ†æ¨ç†è®­ç»ƒçš„å‚æ•°åŒ–é©¬å°”å¯å¤«é“¾ï¼Œä»¥åœ¨æœ‰é™æ—¶é—´åç”Ÿæˆä¸æ•°æ®åŒ¹é…çš„æ ·æœ¬ã€‚è¯¥é“¾çš„è½¬æ¢è¢«å­¦ä¹ ä»¥é€†è½¬æ‰©æ•£è¿‡ç¨‹ï¼Œè¿™æ˜¯ä¸€ä¸ªé©¬å°”å¯å¤«é“¾ï¼Œåœ¨é‡‡æ ·çš„ç›¸åæ–¹å‘ä¸Šé€æ¸å‘æ•°æ®æ·»åŠ å™ªå£°ï¼Œç›´åˆ°ä¿¡å·è¢«ç ´åã€‚å½“æ‰©æ•£åŒ…å«å°‘é‡é«˜æ–¯å™ªå£°æ—¶ï¼Œå°†é‡‡æ ·é“¾è½¬æ¢è®¾ç½®ä¸ºæ¡ä»¶é«˜æ–¯å°±è¶³å¤Ÿäº†ï¼Œä»è€Œå…è®¸ç‰¹åˆ«ç®€å•çš„ç¥ç»ç½‘ç»œå‚æ•°åŒ–ã€‚ğŸ”¤ ([p2](zotero://open-pdf/library/items/RZEREA2F?page=2&annotation=DCFW8JM5))

^KEYDCFW8JM5

> <span class="highlight" style="background-color: #aaaaaa">Diffusion models are straightforward to define and efficient to train, but to the best of our knowledge, there has been no demonstration that they are capable of generating high quality samples. We show that diffusion models actually are capable of generating high quality samples, sometimes better than the published results on other types of generative models (Section 4). In addition, we show that a certain parameterization of diffusion models reveals an equivalence with denoising score matching over multiple noise levels during training and with annealed Langevin dynamics during sampling (Section 3.2) [55, 61]. We obtained our best sample quality results using this parameterization (Section 4.2), so we consider this equivalence to be one of our primary contributions.</span>  
> ğŸ”¤æ‰©æ•£æ¨¡å‹å®šä¹‰ç®€å•ä¸”è®­ç»ƒé«˜æ•ˆï¼Œä½†æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå°šæ— è¯æ®è¡¨æ˜å®ƒä»¬èƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡æ ·æœ¬ã€‚æˆ‘ä»¬è¯æ˜æ‰©æ•£æ¨¡å‹å®é™…ä¸Šèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„æ ·æœ¬ï¼Œæœ‰æ—¶æ¯”å…¶ä»–ç±»å‹çš„ç”Ÿæˆæ¨¡å‹å·²å‘å¸ƒçš„ç»“æœæ›´å¥½ï¼ˆç¬¬ 4 èŠ‚ï¼‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œæ‰©æ•£æ¨¡å‹çš„æŸäº›å‚æ•°åŒ–æ­ç¤ºäº†è®­ç»ƒæœŸé—´å¤šä¸ªå™ªå£°æ°´å¹³ä¸Šçš„å»å™ªåˆ†æ•°åŒ¹é…ä»¥åŠé‡‡æ ·æœŸé—´é€€ç«çš„ Langevin åŠ¨åŠ›å­¦çš„ç­‰ä»·æ€§ï¼ˆç¬¬ 3.2 èŠ‚ï¼‰[55, 61]ã€‚æˆ‘ä»¬ä½¿ç”¨æ­¤å‚æ•°åŒ–ï¼ˆç¬¬ 4.2 èŠ‚ï¼‰è·å¾—äº†æœ€ä½³æ ·æœ¬è´¨é‡ç»“æœï¼Œå› æ­¤æˆ‘ä»¬è®¤ä¸ºè¿™ç§ç­‰ä»·æ€§æ˜¯æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®ä¹‹ä¸€ã€‚ğŸ”¤ ([p2](zotero://open-pdf/library/items/RZEREA2F?page=2&annotation=QNWVV22F))

^KEYQNWVV22F

> <span class="highlight" style="background-color: #ffd400">pÎ¸(x0) := âˆ« pÎ¸(x0:T ) dx1:T</span>  
> The equation you provided is a definition of the marginal probability density function, denoted as p_Î¸(x_0), for a specific variable x_0 in a continuous-time dynamic model. Let's break down the components of the equation:  
> - p_Î¸(x_0:T): This represents the joint probability density function of the variables x_0 through x_T, conditioned on the parameter values Î¸. It describes the probability distribution of the entire sequence of variables from x_0 to x_T.  
> - dğ±_1:T: This represents an integration measure, indicating that we are integrating over the variables x_1 through x_T. It implies that we are integrating out these variables to obtain the marginal probability density function of x_0 alone.  
> - âˆ« p_Î¸(x_0:T) dğ±_1:T: This is the integration of the joint probability density function over the variables x_1 through x_T. By integrating, we obtain the probability of observing x_0 in isolation.  
> - p_Î¸(x_0): This represents the marginal probability density function of x_0 alone, which is obtained by integrating out the other variables.  
> In essence, the equation is defining how to obtain the probability distribution of x_0 alone by integrating the joint probability distribution over all the other variables. This is useful for understanding the probability distribution of individual variables in a dynamic system.  
> ğŸ”¤ğŸ”¤ ([p2](zotero://open-pdf/library/items/RZEREA2F?page=2&annotation=MQAPEUMD))

^KEYMQAPEUMD

> <span class="highlight" style="background-color: #ffd400">variational bound</span>  
> ğŸ”¤å˜åˆ†ç•ŒğŸ”¤ ([p2](zotero://open-pdf/library/items/RZEREA2F?page=2&annotation=2Q877BNK))

^KEY2Q877BNK

> <span class="highlight" style="background-color: #ffd400">negative log likelihood</span>  
> Negative log likelihood (NLL) is a mathematical measurement in statistics that is commonly used in maximum likelihood estimation (MLE). It is the negation of the logarithm of the likelihood function, which measures the probability of observing a set of data given a statistical model and its parameter values.  
> Formally, the negative log likelihood can be defined as:  
> NLL(Î¸) = -log(L(Î¸))  
> Where NLL is the negative log likelihood, Î¸ represents the parameters of the statistical model, and L(Î¸) is the likelihood function. The likelihood function provides the probability of observing the given data under a specific set of parameter values.  
> The NLL is often used as a loss function to optimize the parameters of a model during the training process. The goal is to minimize the NLL, which indirectly maximizes the likelihood and improves the model's fit to the data.  
> In summary, the negative log likelihood is a measure of the discrepancy between the observed data and the predictions of a statistical model, and it is commonly used in maximum likelihood estimation and model training.  
> ğŸ”¤è´Ÿå¯¹æ•°ä¼¼ç„¶ğŸ”¤ ([p2](zotero://open-pdf/library/items/RZEREA2F?page=2&annotation=9HAXZ52Q))

^KEY9HAXZ52Q

> <span class="highlight" style="background-color: #ffd400">q(xt|x0) = N (xt; âˆš Ì„ Î±tx0, (1 âˆ’ Ì„ Î±t)I)</span>  
> è¿™ä¸€æ­¥æ˜¯é€šè¿‡å¯¹äºæ­£å‘è¿‡ç¨‹è¿›è¡Œæ¡ä»¶æ¦‚ç‡çš„å»ºæ¨¡å¾—å‡ºçš„ã€‚æ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸­çš„æ–¹ç¨‹ï¼ˆ4ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°ï¼š  
> $$q(\mathbf{x}_{t}|\mathbf{x}_{0})=\mathcal{N}(\mathbf{x}_{t};\sqrt{\bar{\alpha}_{t}}\mathbf{x}_{0},(1-\bar{\alpha}_{t})\mathbf{I})  $$
> å…¶ä¸­ï¼ŒÎ±_tå’ŒÌ„Î±_tåˆ†åˆ«å®šä¹‰ä¸ºï¼š  
> Î±_t=1-Î²_t  
> Ì„Î±_t =\prod_{t}^{s=1} Î±_s  
> è¿™ä¸ªå»ºæ¨¡æ˜¯ä¸ºäº†ç¡®ä¿æ­£å‘è¿‡ç¨‹å’Œåå‘è¿‡ç¨‹åœ¨Î²_tè¾ƒå°æ—¶å…·æœ‰ç›¸åŒçš„å‡½æ•°å½¢å¼[53]ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ä»»æ„æ—¶é—´æ­¥éª¤tä¸­ä»¥é—­åˆå½¢å¼å¯¹x_tè¿›è¡Œé‡‡æ ·ã€‚ ([p2](zotero://open-pdf/library/items/RZEREA2F?page=2&annotation=KVTXC65K))

^KEYKVTXC65K

> <span class="highlight" style="background-color: #ffd400">Rao-Blackwellized fashion</span>  
> Rao-Blackwellization is a statistical technique that aims to improve the efficiency of an estimator by using conditional expectations. It is named after C. Radhakrishna Rao and David Blackwell, who initially developed and popularized the approach.  
> In the context of fashion, Rao-Blackwellization could refer to the application of this statistical technique to improve the accuracy of trend analysis, forecasting, or personalization within the fashion industry.  
> For example, in trend analysis, Rao-Blackwellization could be used to refine the prediction of future fashion trends by incorporating additional relevant variables or adjusting estimators based on conditional expectations. This would result in a more efficient and accurate forecast compared to traditional methods.  
> Similarly, in personalization, Rao-Blackwellization could help in providing more accurate fashion recommendations to individual customers. By utilizing conditional expectations, the system can make more informed predictions about a customer's preferences based on their previous behavior or the behavior of similar customers.  
> Overall, "Rao-Blackwellized fashion" refers to the application of this statistical technique to enhance the efficiency and accuracy of various fashion-related analyses and recommendations.  
> ğŸ”¤é»‘åŒ–æ—¶è£…ğŸ”¤ ([p3](zotero://open-pdf/library/items/RZEREA2F?page=3&annotation=ZKS2LAWY))

^KEYZKS2LAWY

æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯,æˆ‘ä»¬å¯ä»¥æ¨å¯¼å‡ºæ‰€ç»™è¡¨è¾¾å¼çš„è¯¦ç»†è¿‡ç¨‹ã€‚é¦–å…ˆ,æˆ‘ä»¬ç»™å‡ºä»¥ä¸‹çš„å‡å€¼æ–¹å·®çš„è¡¨ç¤ºå½¢å¼:

$$
\mu_\theta(x_t,t) = x_t - \sqrt{(1-\alpha_t)}\phi_\theta(x_t)$$

$$
\Sigma_\theta(x_t,t) = \beta_t\phi^2_\theta(x_t)$$

å…¶ä¸­$\phi_\theta$ä»£è¡¨ä¸€ä¸ªå¯å­¦ä¹ çš„éçº¿æ€§å‡½æ•°[2]ã€‚æ ¹æ®ä¸Šä¸€æ­¥éª¤çš„ç»“æœ,æˆ‘ä»¬å¯ä»¥å†™å‡ºæ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ:

$$
p_\theta(x_{t-1}|x_t) = \mathcal{N}(x_{t-1};\mu_\theta(x_t,t),\Sigma_\theta(x_t,t)) = \mathcal{N}(x_{t-1};x_t - \sqrt{(1-\alpha_t)}\phi_\theta(x_t),\beta_t\phi^2_\theta(x_t)) = p(x_{t-1}|x_t)$$

æ¥ä¸‹æ¥,æˆ‘ä»¬å¯ä»¥ä½¿ç”¨é€’å½’ Substitution æ–¹æ³•æ¨å¯¼å‡º $q(x_t|x_0)$ çš„è¡¨è¾¾å¼[5]ã€‚æˆ‘ä»¬ä» $q(x_1|x_0)$ å¼€å§‹,å°†å…¶è¡¨ç¤ºä¸º:

$$
q(x_1|x_0) = \int p(x_1,x_0)\mathrm{d}x_0 = \int p(x_1|x_0)p(x_0)\mathrm{d}x_0$$

æ ¹æ®ä¸Šä¸€æ­¥çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒçš„ç»“æœ:

$$
p(x_1|x_0) = \mathcal{N}(x_1;\sqrt{(1-\alpha_1)}x_0+\sqrt{\alpha_1}\xi_1,\beta_1\xi_1(x_0)) = \mathcal{N}(x_1;\sqrt{(1-\alpha_1)}x_0,\beta_1 I)$$
å…¶ä¸­$\xi_1$æ˜¯åœ¨$\mathcal{N}(0,I)$ä¸Šé‡‡æ ·çš„å™ªå£°[3]ã€‚è¿›ä¸€æ­¥æ¨å¯¼,å¯¹äºä»»æ„çš„$t>1$,æˆ‘ä»¬æœ‰:

$$
q(x_t|x_0) = \int p(x_t,x_{t-1}\dots x_1,x_0)\mathrm{d}x_1\mathrm{d}x_2\dots\mathrm{d}x_{t-1} = \int \prod^{t-1}_{i=1} p(x_i|x_{i+1})p(x_1)p(x_0)\mathrm{d}x_1\mathrm{d}x_2\dots\mathrm{d}x_{t-1}$$

æ ¹æ®ä¹‹å‰çš„ç»™å®šæ¡ä»¶æ¦‚ç‡åˆ†å¸ƒç»“æœ,æˆ‘ä»¬å¯ä»¥å¾—åˆ°:

$$
q(x_t|x_0) = \int \prod^{t-1}_{i=1} \mathcal{N}(x_i|x_{i+1},\beta_i I)\mathcal{N}(x_1;\sqrt{(1-\alpha_1)}x_0,\beta_1 I)\mathrm{d}x_1\mathrm{d}x_2\dots\mathrm{d}x_{t-1}$$

ä½¿ç”¨ Gaussian æ¦‚ç‡å¯†åº¦å‡½æ•°çš„æ€§è´¨,æˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥è¿›è¡Œè®¡ç®—åŒ–ç®€:

$$
q(x_t|x_0) = \int \mathcal{N}(x_t|\sqrt{(1-\alpha_1)}x_0,\beta_i I)\prod^{t-2}_{i=1} \mathcal{N}(x_i|x_{i+1},\beta_i I)\mathrm{d}x_1\mathrm{d}x_2\dots\mathrm{d}x_{t-1}$$

ç»§ç»­ç®€åŒ–ä¸Šè¿°è¡¨è¾¾å¼,æˆ‘ä»¬å¯ä»¥å¾—åˆ°æ‰€ç»™çš„ $q(x_t|x_0)$ çš„è¡¨è¾¾å¼:

$$
q(x_t|x_0) = \mathcal{N}(x_t;\sqrt{\bar{\alpha}_t}x_0,(1-\bar{\alpha}_t)I)$$

è¿™å°±æ˜¯æ‰€æ±‚çš„ç»“æœã€‚è¯·æ³¨æ„,ä¸Šè¿°è®¡ç®—è¿‡ç¨‹ä¸­çš„ç¬¦å·å«ä¹‰

å…¶ä¸­ï¼Œé€’å½’æ›¿ä»£æ–¹æ³•ï¼ˆRecursive Substitution Methodï¼‰æ˜¯ä¸€ç§è§£å†³é€’å½’æ–¹ç¨‹çš„æ–¹æ³•ã€‚é€’å½’æ–¹ç¨‹æ˜¯ä¸€ç§å®šä¹‰ä¸€ä¸ªæ•°åˆ—æˆ–å‡½æ•°çš„æ–¹ç¨‹ï¼Œå®ƒåŒ…å«äº†è¯¥æ•°åˆ—æˆ–å‡½æ•°çš„å‰ä¸€é¡¹æˆ–å‰å‡ é¡¹ã€‚é€’å½’æ›¿ä»£æ–¹æ³•é€šè¿‡åå¤ä½¿ç”¨é€’å½’æ–¹ç¨‹æ¥é€æ­¥è®¡ç®—å‡ºæ•°åˆ—æˆ–å‡½æ•°çš„å€¼ã€‚

é€’å½’æ›¿ä»£æ–¹æ³•çš„æ­¥éª¤å¦‚ä¸‹ï¼š

1. æ ¹æ®é€’å½’æ–¹ç¨‹ï¼Œç¡®å®šåˆå§‹æ¡ä»¶ï¼ˆåˆå§‹é¡¹ï¼‰ã€‚
2. åˆ©ç”¨é€’å½’æ–¹ç¨‹ï¼Œå°†å‰ä¸€é¡¹çš„å€¼æ›¿ä»£ä¸ºåä¸€é¡¹çš„è¡¨è¾¾å¼ã€‚
3. é‡å¤æ­¥éª¤2ï¼Œç›´åˆ°è®¡ç®—å‡ºæ‰€éœ€çš„é¡¹ã€‚

é€’å½’æ›¿ä»£æ–¹æ³•å¸¸ç”¨äºè§£å†³é€’å½’æ–¹ç¨‹çš„æ•°å­¦é—®é¢˜ï¼Œä¾‹å¦‚è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—ã€é˜¶ä¹˜ç­‰ã€‚é€šè¿‡é€’å½’æ›¿ä»£æ–¹æ³•ï¼Œå¯ä»¥é€æ­¥è®¡ç®—å‡ºæ•°åˆ—æˆ–å‡½æ•°çš„å„é¡¹çš„å€¼ï¼Œä»è€Œå¾—åˆ°è§£å†³é—®é¢˜çš„ç»“æœã€‚