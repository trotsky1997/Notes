BiZO: Effective Zero-th order Model Optimisation Via Alternative Bi-directional Layer Simultaneous Perturbing

In this blog post, we present a novel method for optimising deep neural networks without using gradient information. Our method, called BiZO, is based on the idea of bi-directional layer simultaneous perturbing (BLSP), which randomly perturbs the weights of two layers in opposite directions and evaluates the change in the objective function. By doing this, we can estimate the sign of the gradient for each layer and update the weights accordingly. BiZO is a zero-th order method, meaning that it does not require access to the gradient or the loss function, only to the model outputs. This makes it suitable for black-box optimisation problems, such as adversarial attacks or hyperparameter tuning.

We compare BiZO with several state-of-the-art zero-th order optimisation methods on various tasks, such as image classification, natural language processing, and reinforcement learning. We show that BiZO can achieve comparable or better performance than existing methods, while being more efficient and robust. We also provide theoretical analysis and empirical evidence to support our claims. We hope that our work will inspire more research on zero-th order optimisation methods for deep learning and artificial intelligence.
